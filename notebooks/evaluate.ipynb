{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aedb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85aba033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d3ee6",
   "metadata": {},
   "source": [
    "## SUPPORT FUNCTIONS (METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "236705c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "prompt_methods = ['zero-shot', 'few-shot-project', 'few-shot-bm25', 'few-shot-codeBERT', 'cot', 'critique', 'expert']\n",
    "\n",
    "def compute_bleu(res, data, method='zero-shot'):\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    project_scores = []\n",
    "    chencherry = SmoothingFunction()\n",
    "    for i in range(0, len(res)):\n",
    "        candidate_list_project = []\n",
    "        reference_list_project = []\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = word_tokenize(res[i]['functions_res'][j][method])#.split()\n",
    "            reference = word_tokenize(data[i]['functions'][j]['docstring'])#.split()\n",
    "            candidate_list.append(candidate)\n",
    "            reference_list.append([reference])\n",
    "            candidate_list_project.append(candidate)\n",
    "            reference_list_project.append([reference])\n",
    "        project_scores.append(corpus_bleu(reference_list_project, candidate_list_project, smoothing_function=chencherry.method0))\n",
    "    bleu_score = corpus_bleu(reference_list, candidate_list, smoothing_function=chencherry.method0)\n",
    "    bleu_score_per_project = sum(project_scores) / len(res)\n",
    "    print(f\"BLEU Score ({method}):\", bleu_score, \", average by project:\", bleu_score_per_project)\n",
    "    return bleu_score, bleu_score_per_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35c72f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/quanvo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/quanvo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/quanvo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "def compute_meteor(res, data, method=\"zero-shot\"):\n",
    "    scores = []\n",
    "    project_scores = []\n",
    "    for i in range(0, len(res)):\n",
    "        cur_scores = []\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = word_tokenize(res[i]['functions_res'][j][method])#.split()\n",
    "            reference = word_tokenize(data[i]['functions'][j]['docstring'])#.split()\n",
    "            score = round(meteor([reference], candidate), 4)\n",
    "            scores.append(score)\n",
    "            cur_scores.append(score)\n",
    "        cur_scores = sum(cur_scores) / len(cur_scores)\n",
    "        project_scores.append(cur_scores)\n",
    "    meteor_score = sum(scores) / len(scores)\n",
    "    meteor_score_per_project = sum(project_scores) / len(res)\n",
    "    print(f\"METEOR Score ({method}):\", meteor_score, \", average by project:\", meteor_score_per_project)\n",
    "    return meteor_score, meteor_score_per_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fbcdc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rougel(res, data, method=\"zero-shot\"):\n",
    "    #CODE BASED ON RENCOS: https://github.com/zhangj111/rencos/blob/master/evaluation/rouge/rouge.py\n",
    "    def my_lcs(string, sub):\n",
    "        \"\"\"\n",
    "        Calculates longest common subsequence for a pair of tokenized strings\n",
    "        :param string : list of str : tokens from a string split using whitespace\n",
    "        :param sub : list of str : shorter string, also split using whitespace\n",
    "        :returns: length (list of int): length of the longest common subsequence between the two strings\n",
    "\n",
    "        Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n",
    "        \"\"\"\n",
    "        if(len(string)< len(sub)):\n",
    "            sub, string = string, sub\n",
    "\n",
    "        lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
    "\n",
    "        for j in range(1,len(sub)+1):\n",
    "            for i in range(1,len(string)+1):\n",
    "                if(string[i-1] == sub[j-1]):\n",
    "                    lengths[i][j] = lengths[i-1][j-1] + 1\n",
    "                else:\n",
    "                    lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
    "\n",
    "        return lengths[len(string)][len(sub)]\n",
    "\n",
    "    def calc_score(candidate, refs, beta=1.2):\n",
    "        \"\"\"\n",
    "        Compute ROUGE-L score given one candidate and references for an image\n",
    "        :param candidate: str : candidate sentence to be evaluated\n",
    "        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n",
    "        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n",
    "        \"\"\"\n",
    "        assert(len(candidate)==1)\n",
    "        assert(len(refs)>0)         \n",
    "        prec = []\n",
    "        rec = []\n",
    "\n",
    "        # split into tokens\n",
    "        token_c = candidate[0].split(\" \")\n",
    "\n",
    "        for reference in refs:\n",
    "            # split into tokens\n",
    "            token_r = reference.split(\" \")\n",
    "            # compute the longest common subsequence\n",
    "            lcs = my_lcs(token_r, token_c)\n",
    "            prec.append(lcs/float(len(token_c)))\n",
    "            rec.append(lcs/float(len(token_r)))\n",
    "\n",
    "        prec_max = max(prec)\n",
    "        rec_max = max(rec)\n",
    "\n",
    "        if(prec_max!=0 and rec_max !=0):\n",
    "            score = ((1 + beta**2)*prec_max*rec_max)/float(rec_max + beta**2*prec_max)\n",
    "        else:\n",
    "            score = 0.0\n",
    "        return score\n",
    "    \n",
    "    scores = []\n",
    "    project_scores = []\n",
    "    for i in range(0, len(res)):\n",
    "        cur_scores = []\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = res[i]['functions_res'][j][method]\n",
    "            reference = data[i]['functions'][j]['docstring']\n",
    "            score = round(calc_score([candidate], [reference]), 4)\n",
    "            scores.append(score)\n",
    "            cur_scores.append(score)\n",
    "        cur_scores = sum(cur_scores) / len(cur_scores)\n",
    "        project_scores.append(cur_scores)\n",
    "    rougel_score = sum(scores) / len(scores)\n",
    "    rougel_score_per_project = sum(project_scores) / len(project_scores)\n",
    "    print(f\"ROUGE-L Score ({method}):\", rougel_score, \", average by project:\", rougel_score_per_project)\n",
    "    return rougel_score, rougel_score_per_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0221fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "def compute_bertscore(res, data, method=\"zero-shot\"):\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    project_scores = []\n",
    "\n",
    "    #GLOBAL\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = res[i]['functions_res'][j][method]\n",
    "            reference = data[i]['functions'][j]['docstring']\n",
    "            candidate_list.append(candidate)\n",
    "            reference_list.append(reference)\n",
    "    bert_score = bertscore.compute(predictions=candidate_list, references=reference_list, lang=\"en\")\n",
    "    f1 = sum(bert_score['f1']) / len(bert_score['f1'])\n",
    "    # PER PROJECT\n",
    "    k = 0\n",
    "    for i in range(0, len(res)):\n",
    "        score = []\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            score.append(bert_score['f1'][k])\n",
    "            k += 1\n",
    "        score = sum(score) / len(score)\n",
    "        project_scores.append(score)\n",
    "    bert_score_per_project = sum(project_scores) / len(project_scores)\n",
    "    \n",
    "    print(f\"BERT Score F1 ({method}):\", f1, \", average by project:\", bert_score_per_project)\n",
    "    return bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1133d803-0217-4dfa-8856-67883dad51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "def compute_sentencebert(res, data, method=\"zero-shot\"):\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    project_scores_cos = []\n",
    "    project_scores_euc = []\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = res[i]['functions_res'][j][method]\n",
    "            reference = data[i]['functions'][j]['docstring']\n",
    "            candidate_list.append(candidate)\n",
    "            reference_list.append(reference)\n",
    "    #COSINE SIMILARITY\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\", similarity_fn_name=SimilarityFunction.COSINE)\n",
    "    candidate_embeddings = model.encode(candidate_list)\n",
    "    reference_embeddings = model.encode(reference_list)\n",
    "    # PER PROJECT COSINE SIMILARITY\n",
    "    k = 0\n",
    "    for i in range(0, len(res)):\n",
    "        candidate_embeddings_project = []\n",
    "        reference_embeddings_project = []\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate_embeddings_project.append(candidate_embeddings[k])\n",
    "            reference_embeddings_project.append(reference_embeddings[k])\n",
    "            k += 1\n",
    "        cos_sim_pairwise = model.similarity(candidate_embeddings_project, reference_embeddings_project)\n",
    "        cos_sim = 0\n",
    "        for i in range(0, len(res[i]['functions_res'])):\n",
    "            cos_sim += cos_sim_pairwise[i, i]\n",
    "        cos_sim /= len(candidate_embeddings_project)\n",
    "        project_scores_cos.append(cos_sim)\n",
    "    # GLOBAL COSINE SIMILARITY\n",
    "    cos_sim_pairwise = model.similarity(candidate_embeddings, reference_embeddings)\n",
    "    cos_sim = 0\n",
    "\n",
    "    # EUCLIDEAN\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\", similarity_fn_name=SimilarityFunction.EUCLIDEAN)\n",
    "    # PER PROJECT EUCLIDEAN SIMILARITY\n",
    "    k = 0\n",
    "    for i in range(0, len(res)):\n",
    "        candidate_embeddings_project = []\n",
    "        reference_embeddings_project = []\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate_embeddings_project.append(candidate_embeddings[k])\n",
    "            reference_embeddings_project.append(reference_embeddings[k])\n",
    "            k += 1\n",
    "        euclidean_sim_pairwise = model.similarity(candidate_embeddings_project, reference_embeddings_project)\n",
    "        euclidean_sim = 0\n",
    "        for i in range(0, len(res[i]['functions_res'])):\n",
    "            euclidean_sim += euclidean_sim_pairwise[i, i]\n",
    "        euclidean_sim /= len(candidate_embeddings_project)\n",
    "        project_scores_euc.append(euclidean_sim)\n",
    "\n",
    "    # GLOBAL EUCLIDEAN SIMILARITY\n",
    "    euclidean_sim_pairwise = model.similarity(candidate_embeddings, reference_embeddings)\n",
    "    euclidean_sim = 0\n",
    "    for i in range(0, len(candidate_list)):\n",
    "        euclidean_sim += euclidean_sim_pairwise[i,i]\n",
    "        cos_sim += cos_sim_pairwise[i,i]\n",
    "    euclidean_sim /= len(candidate_list)\n",
    "    cos_sim /= len(candidate_list)\n",
    "    euclidean_sim_per_project = sum(project_scores_euc) / len(project_scores_euc)\n",
    "    cos_sim_per_project = sum(project_scores_cos) / len(project_scores_cos)\n",
    "    print(f\"SentenceBert euclidean similarity ({method}):\", euclidean_sim.item(), \", average by project:\", euclidean_sim_per_project.item())\n",
    "    print(f\"SentenceBert cosine similarity ({method}):\", cos_sim.item(), \", average by project:\", cos_sim_per_project.item())\n",
    "    return euclidean_sim, cos_sim, euclidean_sim_per_project, cos_sim_per_project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "578b50b7-a742-46a4-8e6b-b51b839c4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def compute_USE(res, data, method='zero-shot'):\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "    model = hub.load(module_url)\n",
    "    def embed(input):\n",
    "      return model(input)\n",
    "    compute_cos_sim = lambda a, b: dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    project_scores = []\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = res[i]['functions_res'][j][method]\n",
    "            reference = data[i]['functions'][j]['docstring']\n",
    "            candidate_list.append(candidate)\n",
    "            reference_list.append(reference)\n",
    "    candidate_embedding = np.array(embed(candidate_list)).tolist()\n",
    "    reference_embedding = np.array(embed(reference_list)).tolist()\n",
    "\n",
    "    k = 0\n",
    "    for i in range(0, len(res)):\n",
    "        cur_score = 0\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = candidate_embedding[k]\n",
    "            reference = reference_embedding[k]\n",
    "            cur_score += compute_cos_sim(candidate, reference)\n",
    "            k += 1\n",
    "        cur_score /= len(res[i]['functions_res'])\n",
    "        project_scores.append(cur_score)\n",
    "    use_score_per_project = sum(project_scores) / len(res)\n",
    "\n",
    "    use_score = 0\n",
    "    for i in range(0, len(candidate_list)):\n",
    "        candidate = candidate_embedding[i]\n",
    "        reference = reference_embedding[i]\n",
    "        use_score += compute_cos_sim(candidate, reference)\n",
    "    use_score /= len(candidate_list)\n",
    "    print(f\"Universal Sentence Encoder Cosine Similarity: ({method}):\", use_score, \", average by project:\", use_score_per_project)\n",
    "    return use_score, use_score_per_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c458dbe",
   "metadata": {},
   "source": [
    "# JAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7a63924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD GENERATED RESULT\n",
    "res = []\n",
    "path = 'data'\n",
    "file_name = 'res_java.jsonl'\n",
    "file_path = os.path.join(parent_dir, path, file_name)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            res.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8dfaaeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TEST DATA\n",
    "data = []\n",
    "path = 'data'\n",
    "file_name = 'java-test-train-small.jsonl'\n",
    "file_path = os.path.join(parent_dir, path, file_name)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            data.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "144d6149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['code', 'code_tokens', 'docstring', 'project', 'name', 'code_tokens_processed', 'bm25', 'CodeBERT'])\n",
      "dict_keys(['zero-shot', 'few-shot-project', 'few-shot-codeBERT', 'cot', 'critique', 'few-shot-bm25', 'expert'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['functions'][0].keys())\n",
    "print(res[0]['functions_res'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b661302",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f32f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score (zero-shot): 0.016874090156521204 , average by project: 0.013233802055073034\n",
      "BLEU Score (few-shot-project): 0.03584189326003828 , average by project: 0.049089874196136474\n",
      "BLEU Score (few-shot-bm25): 0.02243248381754831 , average by project: 0.022016782167999457\n",
      "BLEU Score (few-shot-codeBERT): 0.029398894297030725 , average by project: 0.02881632316649521\n",
      "BLEU Score (cot): 0.011396419813513766 , average by project: 0.008050624423061611\n",
      "BLEU Score (critique): 0.011497885679768026 , average by project: 0.008528160291698887\n",
      "BLEU Score (expert): 0.024701784595697784 , average by project: 0.020031717196577127\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_bleu(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2cd53",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e98b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score (zero-shot): 0.21896869032350025 , average by project: 0.20949970035452117\n",
      "METEOR Score (few-shot-project): 0.26845264357987775 , average by project: 0.29165793751041347\n",
      "METEOR Score (few-shot-bm25): 0.2342813738064698 , average by project: 0.23426783267115214\n",
      "METEOR Score (few-shot-codeBERT): 0.2537084366538404 , average by project: 0.25175977986845494\n",
      "METEOR Score (cot): 0.18621852643579878 , average by project: 0.17471191915004153\n",
      "METEOR Score (critique): 0.17721209918768746 , average by project: 0.16699517353049306\n",
      "METEOR Score (expert): 0.25225680490237995 , average by project: 0.24690678078216\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_meteor(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d83bb9",
   "metadata": {},
   "source": [
    "## ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "838b323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L Score (zero-shot): 0.10532109163460163 , average by project: 0.0979719950133674\n",
      "ROUGE-L Score (few-shot-project): 0.14004801197092734 , average by project: 0.15305117797474685\n",
      "ROUGE-L Score (few-shot-bm25): 0.11471065982613647 , average by project: 0.11225008880629651\n",
      "ROUGE-L Score (few-shot-codeBERT): 0.12691771412284458 , average by project: 0.12004186925996775\n",
      "ROUGE-L Score (cot): 0.09429030924896685 , average by project: 0.08760500621067219\n",
      "ROUGE-L Score (critique): 0.08378399600969066 , average by project: 0.07641002808547133\n",
      "ROUGE-L Score (expert): 0.1238815020664095 , average by project: 0.11532169139159419\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_rougel(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27594452",
   "metadata": {},
   "source": [
    "## BERT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91700b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score F1 (zero-shot): 0.8190588910762819 , average by project: 0.8156653514041078\n",
      "BERT Score F1 (few-shot-project): 0.8199630774832594 , average by project: 0.8184899001565394\n",
      "BERT Score F1 (few-shot-bm25): 0.8187919765247891 , average by project: 0.8149774811369865\n",
      "BERT Score F1 (few-shot-codeBERT): 0.8172474105982966 , average by project: 0.813838841140864\n",
      "BERT Score F1 (cot): 0.8183680412792083 , average by project: 0.815188294083968\n",
      "BERT Score F1 (critique): 0.8121273606204402 , average by project: 0.809092252568307\n",
      "BERT Score F1 (expert): 0.822157006947016 , average by project: 0.8186684723482897\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_bertscore(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dbe0f-d4d0-4ea2-aab2-b3287fd754e0",
   "metadata": {},
   "source": [
    "## SENTENCE BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "425e115b-2ca6-48f9-a274-cb01b5c05cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quanvo/Documents/van-vo-projects/llm-code-comment-gen/llm-code-comment-gen/.venv/lib/python3.10/site-packages/sentence_transformers/util.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  a = torch.tensor(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceBert euclidean similarity (zero-shot): -0.907550573348999 , average by project: tensor(-0.9212)\n",
      "SentenceBert cosine similarity (zero-shot): 0.5725163817405701 , average by project: tensor(0.5594)\n",
      "SentenceBert euclidean similarity (few-shot-project): -0.9024014472961426 , average by project: tensor(-0.8900)\n",
      "SentenceBert cosine similarity (few-shot-project): 0.5658039450645447 , average by project: tensor(0.5726)\n",
      "SentenceBert euclidean similarity (few-shot-bm25): -0.9144586324691772 , average by project: tensor(-0.9138)\n",
      "SentenceBert cosine similarity (few-shot-bm25): 0.5613376498222351 , average by project: tensor(0.5605)\n",
      "SentenceBert euclidean similarity (few-shot-codeBERT): -0.9085538387298584 , average by project: tensor(-0.9121)\n",
      "SentenceBert cosine similarity (few-shot-codeBERT): 0.5611753463745117 , average by project: tensor(0.5570)\n",
      "SentenceBert euclidean similarity (cot): -0.9636720418930054 , average by project: tensor(-0.9748)\n",
      "SentenceBert cosine similarity (cot): 0.5176437497138977 , average by project: tensor(0.5073)\n",
      "SentenceBert euclidean similarity (critique): -0.9899874925613403 , average by project: tensor(-1.0011)\n",
      "SentenceBert cosine similarity (critique): 0.490191787481308 , average by project: tensor(0.4791)\n",
      "SentenceBert euclidean similarity (expert): -0.8498820662498474 , average by project: tensor(-0.8638)\n",
      "SentenceBert cosine similarity (expert): 0.6223868727684021 , average by project: tensor(0.6093)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_sentencebert(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce9987-2f45-4633-b8d4-c1fb37358319",
   "metadata": {},
   "source": [
    "## UNIVERSAL SENTENCE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d8f0764d-945a-4a2d-9f39-2a89b6e11c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Sentence Encoder Cosine Similarity: (zero-shot): 0.4284162148154346 , average by project: 0.4044803945941557\n",
      "Universal Sentence Encoder Cosine Similarity: (few-shot-project): 0.4612214404477187 , average by project: 0.456045630839091\n",
      "Universal Sentence Encoder Cosine Similarity: (few-shot-bm25): 0.44599682883566505 , average by project: 0.43100824068609406\n",
      "Universal Sentence Encoder Cosine Similarity: (few-shot-codeBERT): 0.45319276200601094 , average by project: 0.4338135633976017\n",
      "Universal Sentence Encoder Cosine Similarity: (cot): 0.367630400185942 , average by project: 0.344585915872059\n",
      "Universal Sentence Encoder Cosine Similarity: (critique): 0.3510616932881093 , average by project: 0.32906833971913174\n",
      "Universal Sentence Encoder Cosine Similarity: (expert): 0.4794636586843965 , average by project: 0.4570255705009818\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_USE(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb6899-0449-48d8-ab13-d21c99462aa8",
   "metadata": {},
   "source": [
    "# PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "82d6beca-91f0-4a35-b9be-33051d53f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD GENERATED RESULT\n",
    "res = []\n",
    "path = 'data'\n",
    "file_name = 'res_python.jsonl'\n",
    "file_path = os.path.join(parent_dir, path, file_name)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            res.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "16ed8295-dd3c-4bd3-8883-db34be7fb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TEST DATA\n",
    "data = []\n",
    "path = 'data'\n",
    "file_name = 'python-test-train-small.jsonl'\n",
    "file_path = os.path.join(parent_dir, path, file_name)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            data.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bce951ed-5d31-4d7c-b2aa-b9cfdc46c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['code', 'code_tokens', 'docstring', 'project', 'name', 'code_tokens_processed', 'bm25', 'CodeBERT'])\n",
      "dict_keys(['zero-shot', 'few-shot-project', 'few-shot-bm25', 'few-shot-codeBERT', 'cot', 'critique', 'expert'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['functions'][0].keys())\n",
    "print(res[0]['functions_res'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1617a0a-9cbf-441f-b032-a796344ac3bd",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "51d2ff03-26cc-4943-8fad-86d011a98080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score (zero-shot): 0.009135714279285239 , average by project: 0.007194712177461264\n",
      "BLEU Score (few-shot-project): 0.025773850387974764 , average by project: 0.023659840837820065\n",
      "BLEU Score (few-shot-bm25): 0.011774038346827541 , average by project: 0.011025166442711056\n",
      "BLEU Score (few-shot-codeBERT): 0.01371393496488132 , average by project: 0.010923028937304582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quanvo/Documents/van-vo-projects/llm-code-comment-gen/llm-code-comment-gen/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score (cot): 0.008740850561325569 , average by project: 0.0063465703737370215\n",
      "BLEU Score (critique): 0.007218017940833739 , average by project: 0.00499975416535761\n",
      "BLEU Score (expert): 0.013500418938127435 , average by project: 0.010573555970738774\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_bleu(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4f078-e9e7-4a8b-a180-9918696f3c89",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57777a1f-fbb2-4b16-a3eb-40178447bef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score (zero-shot): 0.17187812437512476 , average by project: 0.16937840746435553\n",
      "METEOR Score (few-shot-project): 0.2370252949410128 , average by project: 0.24044231262144627\n",
      "METEOR Score (few-shot-bm25): 0.19833061387722437 , average by project: 0.19948417667357568\n",
      "METEOR Score (few-shot-codeBERT): 0.2000970605878822 , average by project: 0.19966116199925527\n",
      "METEOR Score (cot): 0.1664951009798039 , average by project: 0.16264873694981893\n",
      "METEOR Score (critique): 0.14902243551289746 , average by project: 0.14723480921864998\n",
      "METEOR Score (expert): 0.20001829634073248 , average by project: 0.19693170632801402\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_meteor(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17a374-339b-44a5-bb93-d2971f33b00f",
   "metadata": {},
   "source": [
    "## ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4cd7b2f-ca75-401e-9b0f-75760434372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L Score (zero-shot): 0.0737549490101978 , average by project: 0.07226701042450463\n",
      "ROUGE-L Score (few-shot-project): 0.11504749050189975 , average by project: 0.11668073801715607\n",
      "ROUGE-L Score (few-shot-bm25): 0.08958230353929209 , average by project: 0.0889728344541853\n",
      "ROUGE-L Score (few-shot-codeBERT): 0.0886434113177364 , average by project: 0.08750540820954825\n",
      "ROUGE-L Score (cot): 0.07760591881623684 , average by project: 0.07594554254745678\n",
      "ROUGE-L Score (critique): 0.0626043991201759 , average by project: 0.06097140149628914\n",
      "ROUGE-L Score (expert): 0.0759969806038791 , average by project: 0.07408391580450062\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_rougel(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f90c9-9d66-4d2a-98c4-80089b3eac1c",
   "metadata": {},
   "source": [
    "## BERT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e7a76630-35f5-484c-81f8-0a209d2fad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score F1 (zero-shot): 0.8089383118416257 , average by project: 0.8084108151062861\n",
      "BERT Score F1 (few-shot-project): 0.8128694572989356 , average by project: 0.812612724294526\n",
      "BERT Score F1 (few-shot-bm25): 0.8093283590949123 , average by project: 0.8073308567236545\n",
      "BERT Score F1 (few-shot-codeBERT): 0.8046031195720276 , average by project: 0.8044130419853818\n",
      "BERT Score F1 (cot): 0.8109882023901349 , average by project: 0.8104600911764411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score F1 (critique): 0.8008047559337124 , average by project: 0.8005093521595632\n",
      "BERT Score F1 (expert): 0.7952278106242651 , average by project: 0.7944018327103624\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_bertscore(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f69590-4433-461f-bbe9-aff44c3cb219",
   "metadata": {},
   "source": [
    "## SENTENCE BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "869ad412-5dcd-4e99-ab93-649ecf4c3020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceBert euclidean similarity (zero-shot): -0.982590913772583 , average by project: tensor(-0.9870)\n",
      "SentenceBert cosine similarity (zero-shot): 0.5028473734855652 , average by project: tensor(0.4985)\n",
      "SentenceBert euclidean similarity (few-shot-project): -0.9129919409751892 , average by project: tensor(-0.9081)\n",
      "SentenceBert cosine similarity (few-shot-project): 0.5645687580108643 , average by project: tensor(0.5668)\n",
      "SentenceBert euclidean similarity (few-shot-bm25): -0.9531254172325134 , average by project: tensor(-0.9526)\n",
      "SentenceBert cosine similarity (few-shot-bm25): 0.5321130752563477 , average by project: tensor(0.5323)\n",
      "SentenceBert euclidean similarity (few-shot-codeBERT): -0.9548503160476685 , average by project: tensor(-0.9559)\n",
      "SentenceBert cosine similarity (few-shot-codeBERT): 0.528922975063324 , average by project: tensor(0.5282)\n",
      "SentenceBert euclidean similarity (cot): -0.9914019107818604 , average by project: tensor(-0.9980)\n",
      "SentenceBert cosine similarity (cot): 0.49311044812202454 , average by project: tensor(0.4864)\n",
      "SentenceBert euclidean similarity (critique): -1.037176489830017 , average by project: tensor(-1.0412)\n",
      "SentenceBert cosine similarity (critique): 0.44499772787094116 , average by project: tensor(0.4413)\n",
      "SentenceBert euclidean similarity (expert): -0.9195144176483154 , average by project: tensor(-0.9238)\n",
      "SentenceBert cosine similarity (expert): 0.5646584630012512 , average by project: tensor(0.5605)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_sentencebert(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4657f5f-56d5-49bb-9433-7c391db6c1ab",
   "metadata": {},
   "source": [
    "## UNIVERSAL SENTENCE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4011e1cc-ed12-4cc6-a879-2a78922eccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Sentence Encoder Cosine Similarity: (zero-shot): 0.3320116902873427 , average by project: 0.3340418949858086\n",
      "Universal Sentence Encoder Cosine Similarity: (few-shot-project): 0.4375413893169157 , average by project: 0.44132198794163524\n",
      "Universal Sentence Encoder Cosine Similarity: (few-shot-bm25): 0.4093193930521929 , average by project: 0.41336426823032774\n",
      "Universal Sentence Encoder Cosine Similarity: (few-shot-codeBERT): 0.4106005694359953 , average by project: 0.41411143963281416\n",
      "Universal Sentence Encoder Cosine Similarity: (cot): 0.329668331338655 , average by project: 0.3284375239602013\n",
      "Universal Sentence Encoder Cosine Similarity: (critique): 0.29909015337150474 , average by project: 0.2977883428698053\n",
      "Universal Sentence Encoder Cosine Similarity: (expert): 0.4206500335883102 , average by project: 0.422800042445348\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_USE(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9e659-dd42-4e55-9d47-d885f9f25ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
