{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aedb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85aba033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d3ee6",
   "metadata": {},
   "source": [
    "## SUPPORT FUNCTIONS (METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236705c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_methods = ['zero-shot', 'few-shot-project', 'few-shot-bm25', 'few-shot-codeBERT', 'cot', 'critique', 'expert']\n",
    "\n",
    "def compute_bleu(res, data, method='zero-shot'):\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = word_tokenize(res[i]['functions_res'][j][method])#.split()\n",
    "            reference = word_tokenize(data[i]['functions'][j]['docstring'])#.split()\n",
    "            candidate_list.append(candidate)\n",
    "            reference_list.append([reference])\n",
    "    bleu_score = corpus_bleu(reference_list, candidate_list)\n",
    "    print(f\"BLEU Score ({method}):\", bleu_score)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c72f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/quanvo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/quanvo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/quanvo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "def compute_meteor(res, data, method=\"zero-shot\"):\n",
    "    scores = []\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = word_tokenize(res[i]['functions_res'][j][method])#.split()\n",
    "            reference = word_tokenize(data[i]['functions'][j]['docstring'])#.split()\n",
    "            score = round(meteor([reference], candidate), 4)\n",
    "            scores.append(score)\n",
    "    meteor_score = sum(scores) / len(scores)\n",
    "    print(f\"METEOR Score ({method}):\", meteor_score)\n",
    "    return meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbcdc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rougel(res, data, method=\"zero-shot\"):\n",
    "    #CODE BASED ON RENCOS: https://github.com/zhangj111/rencos/blob/master/evaluation/rouge/rouge.py\n",
    "    def my_lcs(string, sub):\n",
    "        \"\"\"\n",
    "        Calculates longest common subsequence for a pair of tokenized strings\n",
    "        :param string : list of str : tokens from a string split using whitespace\n",
    "        :param sub : list of str : shorter string, also split using whitespace\n",
    "        :returns: length (list of int): length of the longest common subsequence between the two strings\n",
    "\n",
    "        Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n",
    "        \"\"\"\n",
    "        if(len(string)< len(sub)):\n",
    "            sub, string = string, sub\n",
    "\n",
    "        lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
    "\n",
    "        for j in range(1,len(sub)+1):\n",
    "            for i in range(1,len(string)+1):\n",
    "                if(string[i-1] == sub[j-1]):\n",
    "                    lengths[i][j] = lengths[i-1][j-1] + 1\n",
    "                else:\n",
    "                    lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
    "\n",
    "        return lengths[len(string)][len(sub)]\n",
    "\n",
    "    def calc_score(candidate, refs, beta=1.2):\n",
    "        \"\"\"\n",
    "        Compute ROUGE-L score given one candidate and references for an image\n",
    "        :param candidate: str : candidate sentence to be evaluated\n",
    "        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n",
    "        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n",
    "        \"\"\"\n",
    "        assert(len(candidate)==1)\n",
    "        assert(len(refs)>0)         \n",
    "        prec = []\n",
    "        rec = []\n",
    "\n",
    "        # split into tokens\n",
    "        token_c = candidate[0].split(\" \")\n",
    "\n",
    "        for reference in refs:\n",
    "            # split into tokens\n",
    "            token_r = reference.split(\" \")\n",
    "            # compute the longest common subsequence\n",
    "            lcs = my_lcs(token_r, token_c)\n",
    "            prec.append(lcs/float(len(token_c)))\n",
    "            rec.append(lcs/float(len(token_r)))\n",
    "\n",
    "        prec_max = max(prec)\n",
    "        rec_max = max(rec)\n",
    "\n",
    "        if(prec_max!=0 and rec_max !=0):\n",
    "            score = ((1 + beta**2)*prec_max*rec_max)/float(rec_max + beta**2*prec_max)\n",
    "        else:\n",
    "            score = 0.0\n",
    "        return score\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = res[i]['functions_res'][j][method]\n",
    "            reference = data[i]['functions'][j]['docstring']\n",
    "            score = round(calc_score([candidate], [reference]), 4)\n",
    "            scores.append(score)\n",
    "    rougel_score = sum(scores) / len(scores)\n",
    "    print(f\"ROUGE-L Score ({method}):\", rougel_score)\n",
    "    return rougel_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0221fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "def compute_bertscore(res, data, method=\"zero-shot\"):\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = res[i]['functions_res'][j][method]\n",
    "            reference = data[i]['functions'][j]['docstring']\n",
    "            candidate_list.append(candidate)\n",
    "            reference_list.append(reference)\n",
    "    bert_score = bertscore.compute(predictions=candidate_list, references=reference_list, lang=\"en\")\n",
    "    f1 = sum(bert_score['f1']) / len(bert_score['f1'])\n",
    "    print(f\"BERT Score F1 ({method}):\", f1)\n",
    "    return bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1133d803-0217-4dfa-8856-67883dad51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "def compute_sentencebert(res, data, method=\"zero-shot\"):\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    for i in range(0, len(res)):\n",
    "        for j in range(0, len(res[i]['functions_res'])):\n",
    "            candidate = res[i]['functions_res'][j][method]\n",
    "            reference = data[i]['functions'][j]['docstring']\n",
    "            candidate_list.append(candidate)\n",
    "            reference_list.append(reference)\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\", similarity_fn_name=SimilarityFunction.COSINE)\n",
    "    candidate_embeddings = model.encode(candidate_list)\n",
    "    reference_embeddings = model.encode(reference_list)\n",
    "    cos_sim_pairwise = model.similarity(candidate_embeddings, reference_embeddings)\n",
    "    cos_sim = 0\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\", similarity_fn_name=SimilarityFunction.EUCLIDEAN)\n",
    "    euclidean_sim_pairwise = model.similarity(candidate_embeddings, reference_embeddings)\n",
    "    euclidean_sim = 0\n",
    "    for i in range(0, len(candidate_list)):\n",
    "        euclidean_sim += euclidean_sim_pairwise[i,i]\n",
    "        cos_sim += cos_sim_pairwise[i,i]\n",
    "    euclidean_sim /= len(candidate_list)\n",
    "    cos_sim /= len(candidate_list)\n",
    "    print(f\"SentenceBert euclidean similarity ({method}):\", euclidean_sim.item())\n",
    "    print(f\"SentenceBert cosine similarity ({method}):\", cos_sim.item())\n",
    "    return euclidean_sim, cos_sim\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c458dbe",
   "metadata": {},
   "source": [
    "# JAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a63924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD GENERATED RESULT\n",
    "res = []\n",
    "path = 'data'\n",
    "file_name = 'res_java.jsonl'\n",
    "file_path = os.path.join(parent_dir, path, file_name)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            res.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dfaaeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TEST DATA\n",
    "data = []\n",
    "path = 'data'\n",
    "file_name = 'java-test-train-small.jsonl'\n",
    "file_path = os.path.join(parent_dir, path, file_name)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            data.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144d6149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['code', 'code_tokens', 'docstring', 'project', 'name', 'code_tokens_processed', 'bm25', 'CodeBERT'])\n",
      "dict_keys(['zero-shot', 'few-shot-project', 'few-shot-codeBERT', 'cot', 'critique', 'few-shot-bm25', 'expert'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['functions'][0].keys())\n",
    "print(res[0]['functions_res'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b661302",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f32f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score (zero-shot): 0.016874090156521204\n",
      "BLEU Score (few-shot-project): 0.03584189326003828\n",
      "BLEU Score (few-shot-bm25): 0.02243248381754831\n",
      "BLEU Score (few-shot-codeBERT): 0.029398894297030725\n",
      "BLEU Score (cot): 0.011396419813513766\n",
      "BLEU Score (critique): 0.011497885679768026\n",
      "BLEU Score (expert): 0.024701784595697784\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_bleu(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2cd53",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e98b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score (zero-shot): 0.21896869032350025\n",
      "METEOR Score (few-shot-project): 0.26845264357987775\n",
      "METEOR Score (few-shot-bm25): 0.2342813738064698\n",
      "METEOR Score (few-shot-codeBERT): 0.2537084366538404\n",
      "METEOR Score (cot): 0.18621852643579878\n",
      "METEOR Score (critique): 0.17721209918768746\n",
      "METEOR Score (expert): 0.25225680490237995\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_meteor(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d83bb9",
   "metadata": {},
   "source": [
    "## ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838b323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L Score (zero-shot): 0.10532109163460163\n",
      "ROUGE-L Score (few-shot-project): 0.14004801197092734\n",
      "ROUGE-L Score (few-shot-bm25): 0.11471065982613647\n",
      "ROUGE-L Score (few-shot-codeBERT): 0.12691771412284458\n",
      "ROUGE-L Score (cot): 0.09429030924896685\n",
      "ROUGE-L Score (critique): 0.08378399600969066\n",
      "ROUGE-L Score (expert): 0.1238815020664095\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_rougel(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27594452",
   "metadata": {},
   "source": [
    "## BERT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91700b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score F1 (zero-shot): 0.8190588913565945\n",
      "BERT Score F1 (few-shot-project): 0.8199630769820945\n",
      "BERT Score F1 (few-shot-bm25): 0.818791975913198\n",
      "BERT Score F1 (few-shot-codeBERT): 0.8172474101905692\n",
      "BERT Score F1 (cot): 0.8183680419417653\n",
      "BERT Score F1 (critique): 0.8121273609432244\n",
      "BERT Score F1 (expert): 0.8221570065392887\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_bertscore(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dbe0f-d4d0-4ea2-aab2-b3287fd754e0",
   "metadata": {},
   "source": [
    "## SENTENCE BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425e115b-2ca6-48f9-a274-cb01b5c05cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceBert euclidean similarity (zero-shot): -0.907550573348999\n",
      "SentenceBert cosine similarity (zero-shot): 0.5725163817405701\n",
      "SentenceBert euclidean similarity (few-shot-project): -0.9024014472961426\n",
      "SentenceBert cosine similarity (few-shot-project): 0.5658039450645447\n",
      "SentenceBert euclidean similarity (few-shot-bm25): -0.9144586324691772\n",
      "SentenceBert cosine similarity (few-shot-bm25): 0.5613376498222351\n",
      "SentenceBert euclidean similarity (few-shot-codeBERT): -0.9085538387298584\n",
      "SentenceBert cosine similarity (few-shot-codeBERT): 0.5611753463745117\n",
      "SentenceBert euclidean similarity (cot): -0.9636720418930054\n",
      "SentenceBert cosine similarity (cot): 0.5176437497138977\n",
      "SentenceBert euclidean similarity (critique): -0.9899874925613403\n",
      "SentenceBert cosine similarity (critique): 0.490191787481308\n",
      "SentenceBert euclidean similarity (expert): -0.8498820662498474\n",
      "SentenceBert cosine similarity (expert): 0.6223868727684021\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(prompt_methods)):\n",
    "    compute_sentencebert(res, data, method=prompt_methods[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce9987-2f45-4633-b8d4-c1fb37358319",
   "metadata": {},
   "source": [
    "## UNIVERSAL SENTENCE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6095faa-d962-472a-9878-8f5331037565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
