{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8e0c6b-8b94-40e2-abd2-14c975b4fe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quanvo/.cache/huggingface/datasets\n"
     ]
    }
   ],
   "source": [
    "from datasets import config\n",
    "print(config.HF_DATASETS_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a9a9e-cb48-4a49-b5c9-5c4107866ee1",
   "metadata": {},
   "source": [
    "## PROCESS DATA FOR FINAL EVALUATION<br>\n",
    "In The Vault dataset, there are hundred thousands to millions samples for each programming language. Evaluate LLMs on all samples are prohibitedly costly. To compare different methods with each others, some hundreds (200 per language in LLM4CodeSummarization) are enough.\n",
    "In this project, I want to answer: <br>\n",
    "    <b>RQ1: How effective are in-context learning using LLMs in code comment generation? <br>\n",
    "\tRQ2: Does project-specific context improve the performance of LLMs? <br>\n",
    "\tRQ3: Does retrieval-augmented generation improve the performance of LLMs? <br>\n",
    "\tRQ4: How do LLMs perform for different programming languages? <br></b>\n",
    "To evaluate LLMs for RQ1, RQ2 and RQ3, I collect all functions from 100 different projects, each project needs to have at least 5 functions in The Vault to be considered.\n",
    "data structure look like this: <br>\n",
    "<b>ProjectObject:</b><br>\n",
    "    {\n",
    "    \"project_name\": name of the project, corresponding to \"repo\" in The Vault, <br>\n",
    "    \"functions\": List[FunctionObject]. List of functions belongs to this project<br>\n",
    "    } <br>\n",
    "<b>FunctionObject</b>: <br>\n",
    "    {\n",
    "        \"code\": code of function, corresponding to \"code\" in The Vault, <br>\n",
    "        \"docstring\": full docstring of function, corresponding to \"docstring\" in The Vault, <br>\n",
    "        \"project\": name of the project, corresponding to \"repo\" in The Vault, <br>\n",
    "        \"code_tokens\": list of tokens, corresponding to \"code_tokens\" in The Vault, <br>\n",
    "        \"code_tokens_processed\": list of tokens after some processing for bm25 retrieval, <br>\n",
    "        \"bm25\": list of code and docstring of 5 most similar functions retrieved using bm25, <br>\n",
    "        \"CodeBERT\": list of code and docstring of 5 most similar functions retrieved using CodeBERT embedding, <br>\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0288f-ebac-402d-9dfe-15f4bf945c95",
   "metadata": {},
   "source": [
    "<b>STEP 1: Collect functions from 200 projects</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7277a7b-8cd7-435c-a3ce-0041e8956899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "def the_vault_function_to_json(\n",
    "    split_set=\"test\",\n",
    "    languages=[\"java\"],\n",
    "    streaming=True,\n",
    "    num_proj=None,\n",
    "    num_method=None,\n",
    "    min_num_functions=5, #keep only project with at least min_num_functions functions\n",
    "    write_to_file=False,\n",
    "    output_dir=\"../data\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads and processes functions from The Vault dataset, grouping them by project.\n",
    "\n",
    "    This function fetches code samples from the `Fsoft-AIC/the-vault-function` dataset\n",
    "    using the Hugging Face `datasets` library. It groups functions by project and filters \n",
    "    them based on a minimum number of functions per project. The output can optionally \n",
    "    be saved to disk as JSONL files.\n",
    "\n",
    "    Args:\n",
    "        split_set (str): Dataset split to load (e.g., \"train\", \"train/small\", \"train/medium\").\n",
    "        languages (List[str]): List of programming languages to load (e.g., [\"java\"]).\n",
    "        streaming (bool): Whether to stream the dataset instead of loading fully into memory.\n",
    "        num_proj (int, optional): Maximum number of unique projects to collect. Cannot be used with `num_method`.\n",
    "        num_method (int, optional): Maximum number of individual methods/functions to collect. Cannot be used with `num_proj`.\n",
    "        min_num_functions (int): Minimum number of functions a project must have to be included.\n",
    "        write_to_file (bool): Whether to write the processed data to JSONL files.\n",
    "        output_dir (str): Directory where JSONL files will be saved if `write_to_file` is True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each language to a list of project entries. \n",
    "              Each entry is a dict with keys `\"project_name\"` and `\"functions\"`, \n",
    "              where `\"functions\"` is a list of function metadata.\n",
    "              \n",
    "    Raises:\n",
    "        AssertionError: If both `num_proj` and `num_method` are provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert not (num_proj and num_method), \"Cannot use both num_proj and num_method\"\n",
    "\n",
    "    if write_to_file:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    data_lang = {}\n",
    "    for lang in languages:\n",
    "        dataset = load_dataset(\"Fsoft-AIC/the-vault-function\", \n",
    "                               split_set=[split_set], \n",
    "                               languages=[lang], \n",
    "                               streaming=False,\n",
    "                               trust_remote_code=True)\n",
    "        if split_set == \"train/small\":\n",
    "            split_set = \"train_small\"\n",
    "        elif split_set == \"train/medium\":\n",
    "            split_set = \"train_medium\"\n",
    "        dataset = dataset[split_set]\n",
    "        print(f\"Loaded {lang}, split={split_set}, streaming={streaming}\")\n",
    "\n",
    "        data = []\n",
    "        project_function = []\n",
    "        project = \"\"\n",
    "        proj_counts = defaultdict(int)\n",
    "        seen_projects = set()\n",
    "        total = 0\n",
    "        i = 0\n",
    "        for sample in dataset:\n",
    "            i += 1\n",
    "            #print(f\"{i}: {len(seen_projects)}\")\n",
    "            if num_proj and len(seen_projects) >= num_proj:\n",
    "                break\n",
    "            if num_method and len(data) >= num_method:\n",
    "                break\n",
    "            item = {\n",
    "                \"code\": sample[\"code\"],\n",
    "                \"code_tokens\": sample[\"code_tokens\"],\n",
    "                \"docstring\": sample[\"docstring\"],\n",
    "                \"project\": sample[\"repo\"],\n",
    "                \"name\": sample[\"identifier\"]\n",
    "            }\n",
    "            # Encounter new project: append project_function to data, refresh project_function\n",
    "            if project != item[\"project\"] and project != \"\":\n",
    "                if len(project_function) > min_num_functions:\n",
    "                    data.append({\"project_name\": project, \"functions\": project_function})\n",
    "                    total += len(project_function)\n",
    "                    if num_proj:\n",
    "                        seen_projects.add(project)\n",
    "                project_function = []\n",
    "            project = item[\"project\"]\n",
    "            project_function.append(item)\n",
    "            proj_counts[project] += 1\n",
    "\n",
    "        print(f\"{lang} - collected {total} samples across {len(seen_projects)} projects\")\n",
    "\n",
    "        if write_to_file:\n",
    "            output_path = os.path.join(output_dir, f\"{lang}_{split_set}.jsonl\")\n",
    "            with open(output_path, \"w\") as f:\n",
    "                for item in data:\n",
    "                    f.write(json.dumps(item) + \"\\n\")\n",
    "            print(f\"Saved to {output_path}\")\n",
    "        data_lang[lang] = data\n",
    "    return data_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64938249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "def the_vault_class_to_json(\n",
    "    languages=[\"java\"],\n",
    "    streaming=False,\n",
    "    num_proj=None,\n",
    "    num_class=None,\n",
    "    min_num_classes=5, #keep only project with at least min_num_classes classes\n",
    "    write_to_file=False,\n",
    "    output_dir=\"../data\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads and processes classes from The Vault dataset, grouping them by project.\n",
    "\n",
    "    This function fetches code samples from the `Fsoft-AIC/the-vault-class` dataset\n",
    "    using the Hugging Face `datasets` library. It groups classes by project and filters \n",
    "    them based on a minimum number of classes per project. The output can optionally \n",
    "    be saved to disk as JSONL files.\n",
    "\n",
    "    Args:\n",
    "        split_set (str): Dataset split to load (e.g., \"train\", \"validation\", \"test\").\n",
    "        languages (List[str]): List of programming languages to load (e.g., [\"java\"]).\n",
    "        streaming (bool): Whether to stream the dataset instead of loading fully into memory.\n",
    "        num_proj (int, optional): Maximum number of unique projects to collect. Cannot be used with `num_class`.\n",
    "        num_class (int, optional): Maximum number of individual classes to collect. Cannot be used with `num_proj`.\n",
    "        min_num_classes (int): Minimum number of classes a project must have to be included.\n",
    "        write_to_file (bool): Whether to write the processed data to JSONL files.\n",
    "        output_dir (str): Directory where JSONL files will be saved if `write_to_file` is True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each language to a list of project entries. \n",
    "              Each entry is a dict with keys `\"project_name\"` and `\"classes\"`, \n",
    "              where `\"classes\"` is a list of class metadata.\n",
    "              \n",
    "    Raises:\n",
    "        AssertionError: If both `num_proj` and `num_class` are provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert not (num_proj and num_class), \"Cannot use both num_proj and num_class\"\n",
    "\n",
    "    if write_to_file:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    data_lang = {}\n",
    "    for lang in languages:\n",
    "        dataset = load_dataset(\"/home/quanvo/Documents/van-vo-projects/llm-code-comment-gen/the-vault-class\", \n",
    "                               languages=[lang], \n",
    "                               streaming=streaming,\n",
    "                               trust_remote_code=True)\n",
    "        dataset = dataset['train']\n",
    "        print(f\"Loaded {lang}, streaming={streaming}\")\n",
    "\n",
    "        data = []\n",
    "        project_class = []\n",
    "        project = \"\"\n",
    "        proj_counts = defaultdict(int)\n",
    "        seen_projects = set()\n",
    "        i = 0\n",
    "        total = 0\n",
    "        for sample in dataset:\n",
    "            i += 1\n",
    "            #print(f\"{i}: {len(seen_projects)}\")\n",
    "            if num_proj and len(seen_projects) >= num_proj:\n",
    "                break\n",
    "            if num_class and len(data) >= num_class:\n",
    "                break\n",
    "            item = {\n",
    "                \"code\": sample[\"code\"].replace(sample[\"original_docstring\"], \"\"),\n",
    "                \"code_tokens\": sample[\"code_tokens\"],\n",
    "                \"docstring\": sample[\"docstring\"],\n",
    "                \"project\": sample[\"repo\"],\n",
    "                \"name\": sample[\"identifier\"]\n",
    "            }\n",
    "            # Encounter new project: append project_class to data, refresh project_class\n",
    "            if project != item[\"project\"] and project != \"\":\n",
    "                if len(project_class) > min_num_classes:\n",
    "                    data.append({\"project_name\": project, \"classes\": project_class})\n",
    "                    total += len(project_class)\n",
    "                    if num_proj:\n",
    "                        seen_projects.add(project)\n",
    "                project_class = []\n",
    "            project = item[\"project\"]\n",
    "            project_class.append(item)\n",
    "            proj_counts[project] += 1\n",
    "\n",
    "        print(f\"{lang} - collected {total} samples across {len(seen_projects)} projects\")\n",
    "\n",
    "        if write_to_file:\n",
    "            output_path = os.path.join(output_dir, f\"{lang}_{split_set}.jsonl\")\n",
    "            with open(output_path, \"w\") as f:\n",
    "                for item in data:\n",
    "                    f.write(json.dumps(item) + \"\\n\")\n",
    "            print(f\"Saved to {output_path}\")\n",
    "        data_lang[lang] = data\n",
    "    return data_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ae989b-9b6c-4220-859e-ad712e17c382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded java, streaming=True\n",
      "java - collected 2170 samples across 200 projects\n"
     ]
    }
   ],
   "source": [
    "#option 0: inline, 1: functions, 2: classes\n",
    "option = 2\n",
    "lang = \"java\"\n",
    "\n",
    "if option == 0:\n",
    "    pass\n",
    "elif option == 1:\n",
    "    data_lang = the_vault_function_to_json(split_set=\"test\", \n",
    "                                           languages=[lang], \n",
    "                                           num_proj=200, \n",
    "                                           write_to_file=False)\n",
    "elif option == 2:\n",
    "    data_lang = the_vault_class_to_json(languages=[lang], \n",
    "                                    num_proj=200,\n",
    "                                    streaming=True,\n",
    "                                    write_to_file=False)\n",
    "data = data_lang[lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c481a1a-6d5b-4a09-bd52-28bb8c70bf3d",
   "metadata": {},
   "source": [
    "<b>Step 2: Process code tokens </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108e4e22-fb31-40eb-84e1-cdf95f9d168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import keyword\n",
    "\n",
    "def split_identifier(token):\n",
    "    # Split snake_case and then camelCase\n",
    "    parts = re.split(r'[_]', token)\n",
    "    split_parts = []\n",
    "    for part in parts:\n",
    "        # Split camelCase and PascalCase using lookahead and lookbehind\n",
    "        camel_parts = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?![a-z])', part)\n",
    "        split_parts.extend(camel_parts)\n",
    "    return split_parts\n",
    "\n",
    "JAVA_KEYWORDS = {\n",
    "    'abstract', 'assert', 'boolean', 'break', 'byte', 'case', 'catch', 'char',\n",
    "    'class', 'const', 'continue', 'default', 'do', 'double', 'else', 'enum',\n",
    "    'extends', 'final', 'finally', 'float', 'for', 'goto', 'if', 'implements',\n",
    "    'import', 'instanceof', 'int', 'interface', 'long', 'native', 'new',\n",
    "    'package', 'private', 'protected', 'public', 'return', 'short', 'static',\n",
    "    'strictfp', 'super', 'switch', 'synchronized', 'this', 'throw', 'throws',\n",
    "    'transient', 'try', 'void', 'volatile', 'while', 'true', 'false', 'null'\n",
    "}\n",
    "\n",
    "JAVA_API_COMMON = {\n",
    "    'System', 'out', 'in', 'err', 'print', 'println', 'printf', 'String', 'Integer',\n",
    "    'List', 'ArrayList', 'Map', 'HashMap', 'Set', 'HashSet', 'Math', 'Object',\n",
    "}\n",
    "\n",
    "PUNCTUATIONS = [\"''\", \"'\", \"``\", \"`\", \"-LRB-\", \"-RRB-\", \"-LCB-\", \"-RCB-\", \".\", \"?\", \"!\", \",\", \":\", \"-\", \"--\", \"...\", \";\", \"(\", \")\", \n",
    "               \"[\", \"]\", \"=\", \">\", \"<\", \"+\", \"-\", \"/\", \"*\", \">=\", \"<=\", \"==\", \"+=\", \"-=\", \"/=\", \"*=\"]\n",
    "\n",
    "def is_stopword_java(token):\n",
    "    token_lower = token.lower()\n",
    "    return (token in JAVA_KEYWORDS or token in JAVA_API_COMMON or token in PUNCTUATIONS)\n",
    "\n",
    "def process_tokens_java(tokens):\n",
    "    split_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'^[A-Za-z_]+$', token):\n",
    "            split_tokens.extend(split_identifier(token))\n",
    "        else:\n",
    "            split_tokens.append(token)\n",
    "    split_tokens = [t for t in split_tokens if not is_stopword_java(t)]\n",
    "    return split_tokens\n",
    "\n",
    "#################################PYTHON##############################################\n",
    "PYTHON_KEYWORDS = set(keyword.kwlist)\n",
    "\n",
    "PYTHON_BUILTINS = {\n",
    "    'print', 'input', 'len', 'range', 'open', 'map', 'filter', 'zip', 'int', 'float', 'str',\n",
    "    'list', 'dict', 'set', 'tuple', 'type', 'isinstance', 'enumerate', 'sorted', 'reversed',\n",
    "    'sum', 'min', 'max', 'abs', 'any', 'all', 'bool', 'dir', 'divmod', 'id', 'hex', 'oct',\n",
    "    'ord', 'chr', 'pow', 'round', 'slice', 'vars', 'format', 'next', 'iter'\n",
    "}\n",
    "\n",
    "def is_stopword_python(token):\n",
    "    token_lower = token.lower()\n",
    "    return (token in PYTHON_KEYWORDS or token in PYTHON_BUILTINS or token in PUNCTUATIONS)\n",
    "\n",
    "def process_tokens_python(tokens):\n",
    "    split_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'^[A-Za-z_]+$', token):\n",
    "            split_tokens.extend(split_identifier(token))\n",
    "        else:\n",
    "            split_tokens.append(token)\n",
    "    split_tokens = [t for t in split_tokens if not is_stopword_python(t)]\n",
    "    return split_tokens\n",
    "\n",
    "def process_tokens(tokens, lang):\n",
    "    match lang:\n",
    "        case 'java':\n",
    "            return process_tokens_java(tokens)\n",
    "        case 'python':\n",
    "            return process_tokens_python(tokens)\n",
    "        case _:\n",
    "            print(\"Invalid language\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d1c592-96c6-4971-a968-056a14b3968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if option == 0:\n",
    "    data_type = 'snippet'\n",
    "elif option == 1:\n",
    "    data_type = 'functions'\n",
    "else:\n",
    "    data_type = 'classes'\n",
    "for i in range(0, len(data)):\n",
    "    for j in range(0, len(data[i][data_type])):\n",
    "        tokens = data[i][data_type][j]['code_tokens']\n",
    "        data[i][data_type][j]['code_tokens_processed'] = process_tokens(tokens, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e6fca72-bfa7-42c4-af73-7a18bd7f873b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First',\n",
       " 'Page',\n",
       " 'Loading',\n",
       " 'Partial',\n",
       " 'State',\n",
       " 'Changes',\n",
       " '{',\n",
       " '@',\n",
       " 'Override',\n",
       " 'to',\n",
       " '{',\n",
       " '\"',\n",
       " 'FirstPageLoadingState{}',\n",
       " '\"',\n",
       " '}',\n",
       " '}']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][data_type][0]['code_tokens_processed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01c1e1-d4ca-4ffb-ae8b-cddd742f04ff",
   "metadata": {},
   "source": [
    "<b>Step 3: run bm25 </b> <br>\n",
    "In this step, I use bm25s (https://github.com/xhluca/bm25s/tree/main), for faster bm25. However, the retrieve() method of bm25s does not return indices of retrieved documents, so we cannot find corresponding docstring. Therefore, I have to modify the code. In bm25s/bm25s/__init__.py file,  change \"allowed_return_as = [\"tuple\", \"documents\"]\" on line 685 to \"allowed_return_as = [\"tuple\", \"documents\", \"tuple_custom\", \"documents_custom\"]\". On lines 853-858, insert: <br>\n",
    "elif return_as == \"tuple_custom\": <br>\n",
    "    return Results(documents=retrieved_docs, scores=scores), indices <br>\n",
    "elif return_as == \"documents_custom\": <br>\n",
    "    return retrieved_docs, indices <br>\n",
    "Then, I use indices to get docstring. Below is the code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5812c39-4862-44fb-af09-f8b67b5fc4c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create corpus\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "if option == 0:\n",
    "    pass\n",
    "elif option == 1:\n",
    "    dataset = load_dataset(\"Fsoft-AIC/the-vault-function\", \n",
    "                           split_set=[\"train/small\"], \n",
    "                           languages=[lang], \n",
    "                           streaming=False,\n",
    "                           trust_remote_code=True)\n",
    "    dataset = dataset[\"train_small\"]\n",
    "elif option == 2:\n",
    "    dataset = load_dataset(\"/home/quanvo/Documents/van-vo-projects/llm-code-comment-gen/the-vault-class\", \n",
    "        languages=[lang], \n",
    "        streaming=True,\n",
    "        trust_remote_code=True)\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "# UNCOMMENTED TO RUN\n",
    "print(\"create corpus\")\n",
    "corpus = []\n",
    "corpus_tokens = []\n",
    "i = 0\n",
    "for sample in dataset:\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    corpus.append(sample[\"code\"])\n",
    "    corpus_tokens.append(process_tokens(sample[\"code_tokens\"], lang))\n",
    "# corpus = [sample[\"code\"] for sample in dataset]\n",
    "# corpus_tokens = [process_tokens(sample[\"code_tokens\"], lang) for sample in dataset]\n",
    "\n",
    "print(\"indexing...\")\n",
    "retriever = bm25s.BM25(corpus=corpus)\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "# query_tokens = data[0]['functions'][5]['code_tokens_processed']\n",
    "# res, indices = retriever.retrieve([query_tokens], k=5, return_as=\"tuple_custom\")\n",
    "# docs = res.documents\n",
    "# scores = res.scores\n",
    "# print(f\"Best result (score: {scores[0, 4]:.2f}): {docs[0, 4]}, index: {indices[0, 4]}\")\n",
    "\n",
    "# UNCOMMENTED TO RUN\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i][data_type])):\n",
    "        print(f\"{i}: {j}\")\n",
    "        query_tokens = data[i][data_type][j]['code_tokens_processed']\n",
    "        if option == 1:\n",
    "            res, indices = retriever.retrieve([query_tokens], k=5, return_as=\"tuple_custom\")\n",
    "            indices = indices[0, 0:]\n",
    "        else:\n",
    "            res, indices = retriever.retrieve([query_tokens], k=6, return_as=\"tuple_custom\")\n",
    "            indices = indices[0, 1:]\n",
    "        docs = res.documents\n",
    "        scores = res.scores\n",
    "        retrieved_examples = [{'code': dataset[ind.item()]['code'],\n",
    "                               'docstring': dataset[ind.item()]['docstring'],\n",
    "                               'code_tokens': dataset[ind.item()]['code_tokens'],} for ind in indices]\n",
    "        data[i][data_type][j]['bm25'] = retrieved_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf24c4-1215-4470-b9bb-8a514abebfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '.'\n",
    "output_path = None\n",
    "if option == 1:\n",
    "    output_path = os.path.join(output_dir, f\"{lang}-test-train-small.jsonl\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "elif option == 2:\n",
    "    output_path = os.path.join(output_dir, f\"{lang}-class.jsonl\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e93c1-b716-4cd1-940d-098a634c4704",
   "metadata": {},
   "source": [
    "<b>Step 4: CodeBERT embedding </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9a948-d300-464d-9cf7-985ca629d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"Fsoft-AIC/the-vault-function\", \n",
    "#                        split_set=[\"train/small\"], \n",
    "#                        languages=[lang], \n",
    "#                        streaming=False,\n",
    "#                        trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4d92c-244d-45e1-aa26-18fc43146970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "codes = [item[\"code\"] for item in dataset\n",
    "batch_size = 32\n",
    "\n",
    "# Run through model\n",
    "embeddings = []\n",
    "i = 0\n",
    "print(len(dataset))\n",
    "start = time.time()\n",
    "for i in range(0, len(codes), batch_size):\n",
    "    batch_codes = codes[i:i + batch_size]\n",
    "    print(i)\n",
    "    inputs = tokenizer(batch_codes, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        embeddings.append(embedding.cpu().numpy())\n",
    "end = time.time()\n",
    "print(end - time.time())\n",
    "\n",
    "embeddings_np = np.array(embeddings, dtype=object)\n",
    "embeddings_np = np.vstack(embeddings_np)\n",
    "if option == 1:\n",
    "    pass\n",
    "    #np.save('embedding_func_{lang}_train_small.npy', embeddings_np, allow_pickle=True)\n",
    "elif option == 2:\n",
    "    pass\n",
    "    np.save(f'embedding_class_{lang}.npy', embeddings_np, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3918986-84ae-49dc-aa91-b445453159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "if option == 1:\n",
    "    embeddings_np = np.load(f'embedding_func_{lang}_train_small.npy', allow_pickle=True)\n",
    "elif option == 2:\n",
    "    embeddings_np = np.load(f'embedding_class_{lang}.npy', allow_pickle=True)\n",
    "embeddings_np = np.vstack(embeddings_np)\n",
    "print(embeddings_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be328f4d-b42c-4e48-953a-b57f13671621",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(embeddings_np)\n",
    "dimension = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "if faiss.get_num_gpus() > 0:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "print(\"indexing...\")\n",
    "index.add(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1063d-a07b-4e05-afee-7ccc9ffa6f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_code = data[0][data_type][5]['code']\n",
    "query_input = tokenizer(query_code, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "query_embedding = model(**query_input).last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "k = 5\n",
    "distances, indices = index.search(np.array(query_embedding), k)\n",
    "\n",
    "retrieved_examples = [dataset[i.item()][\"code\"] for i in indices[0]]\n",
    "print(retrieved_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf0eb4-5ae2-4d28-8de6-cc4da8fc4e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i][data_type])):\n",
    "        print(f\"{i}: {j}\")\n",
    "        query_code = data[i][data_type][j]['code']\n",
    "        query_input = tokenizer(query_code, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        query_embedding = model(**query_input).last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "        if option == 1:\n",
    "            k = 5\n",
    "            distances, indices = index.search(np.array(query_embedding), k)\n",
    "        else:\n",
    "            k = 6\n",
    "            distances, indices = index.search(np.array(query_embedding), k)\n",
    "            indices[0] = indices[0, 1:]\n",
    "        retrieved_examples = [{'code': dataset[ind.item()]['code'],\n",
    "                               'docstring': dataset[ind.item()]['docstring'],\n",
    "                               'code_tokens': dataset[ind.item()]['code_tokens'],} for ind in indices[0]]\n",
    "        data[i][data_type][j]['CodeBERT'] = retrieved_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cedd2d-a32d-49e3-a443-d72c0a66604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '.'\n",
    "if option == 1:\n",
    "    output_path = os.path.join(output_dir, f\"{lang}-test-(train_small).jsonl\")\n",
    "elif option == 2:\n",
    "    output_path = os.path.join(output_dir, f\"{lang}-class.jsonl\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "print(f\"Saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
