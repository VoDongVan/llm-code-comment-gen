{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd855fe0-e2e7-479a-8c32-8162e0e562b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af821e2-fba6-4bf1-ade3-9d9343b3f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0087416-8b69-4d20-be54-f3869afe5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = 'data'\n",
    "file_name = 'java-test-train-small.jsonl'\n",
    "file_path = os.path.join(parent_dir, path, file_name)\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            data.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line: {line.strip()} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bed8f-7725-4b3f-8fd9-41c076204bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.llm import load_model, generate_comments\n",
    "from prompts.templates import prepare_prompt\n",
    "\n",
    "# Load model and tokenizer\n",
    "# model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# model, tokenizer = load_model(model_name)\n",
    "print(next(model.parameters()).device)\n",
    "#model = model.to(\"cuda\")\n",
    "#tokenizer = tokenizer.to(\"cuda\")\n",
    "\n",
    "# Sample test input\n",
    "test_input = {\"code\": data[0]['functions'][0]['code']}\n",
    "few_shot_examples = data[0]['functions'][0]['CodeBERT']\n",
    "for example in few_shot_examples:\n",
    "    example['comment'] = example['docstring']\n",
    "\n",
    "# Test different prompt strategies\n",
    "prompt_types = [\"zero-shot\", \"few-shot\", \"cot\", \"critique\", \"expert\"]\n",
    "\n",
    "for prompt_type in prompt_types:\n",
    "    print(f\"\\nTesting {prompt_type} Prompt Strategy:\")\n",
    "    print('===========================')\n",
    "    prompt = prepare_prompt(test_input, data_type=\"function\", prompt_type=prompt_type, few_shot_examples=few_shot_examples)\n",
    "    for p in prompt:\n",
    "        print(f\"{p['role']}: {p['content']}\")\n",
    "        print('=======================')\n",
    "    comments = generate_comments(model, tokenizer, prompt, prompt_type=prompt_type)\n",
    "    print(f\"Generated Comments:\\n{comments}\")\n",
    "    print('=======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0c790-674a-49df-9583-7a813423d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llm import load_model, generate_comments\n",
    "from prompts.templates import prepare_prompt\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "model, tokenizer = load_model(model_name)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f0fbd-3d11-429d-912b-cde0d3f039d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "res = []\n",
    "for project in data:\n",
    "    project_res = {}\n",
    "    project_res[\"project_name\"] = project[\"project_name\"]\n",
    "    project_res[\"functions_res\"] = []\n",
    "    for i in range(0, len(project['functions'])):\n",
    "        # prepare project few-shot examples: take 5 random examples from the same project and let the model predict\n",
    "        project_fs_indices = []\n",
    "        while len(project_fs_indices) < 5:\n",
    "            ind = random.randint(0, len(project['functions']) - 1)\n",
    "            if ind != i and ind not in project_fs_indices:\n",
    "                project_fs_indices.append(ind)\n",
    "        project_fs_examples = []\n",
    "        for ind in project_fs_indices:\n",
    "            example = {}\n",
    "            example['code'] = project['functions'][ind]['code']\n",
    "            example['comment'] = project['functions'][ind]['docstring']\n",
    "            project_fs_examples.append(example)\n",
    "        #prepare codeBERT few-shot examples\n",
    "        codeBERT_examples = project['functions'][i]['CodeBERT']\n",
    "        for example in codeBERT_examples:\n",
    "            example['comment'] = example['docstring']\n",
    "\n",
    "        #Run each prompt type\n",
    "        # Test different prompt strategies\n",
    "        prompt_types = [\"zero-shot\", \"few-shot-project\", \"few-shot-codeBERT\", \"cot\", \"critique\", \"expert\"]\n",
    "        function_res = {}\n",
    "        fs_examples = []\n",
    "        test_input = {\"code\": project['functions'][i]['code']}\n",
    "        for prompt_type in prompt_types:\n",
    "            if prompt_type == \"few-shot-project\":\n",
    "                fs_examples = project_fs_examples\n",
    "            elif prompt_type == \"few-shot-codeBERT\":\n",
    "                fs_examples = codeBERT_examples\n",
    "            print(f\"\\nTesting {prompt_type} Prompt Strategy:\")\n",
    "            print('===========================')\n",
    "            if prompt_type == \"few-shot-project\" or prompt_type == \"few-shot-codeBERT\":\n",
    "                arg_prompt_type = \"few-shot\"\n",
    "            else:\n",
    "                arg_prompt_type = prompt_type\n",
    "            prompt = prepare_prompt(test_input, data_type=\"function\", prompt_type=arg_prompt_type, few_shot_examples=fs_examples)\n",
    "            for p in prompt:\n",
    "                print(f\"{p['role']}: {p['content']}\")\n",
    "                print('=======================')\n",
    "            comments = generate_comments(model, tokenizer, prompt, prompt_type=prompt_type)\n",
    "            function_res[prompt_type] = comments\n",
    "            print(f\"Generated Comments:\\n{comments}\")\n",
    "            print('=======================')\n",
    "        project_res[\"functions_res\"].append(function_res)\n",
    "    res.append(project_res)\n",
    "    break\n",
    "\n",
    "# output_dir = os.path.join(parent_dir, 'data')\n",
    "# output_path = os.path.join(output_dir, f\"java-test-train-small.jsonl\")\n",
    "# with open(output_path, \"w\") as f:\n",
    "#     for item in data:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n",
    "# print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997d8f4-d0d9-4244-bfdb-c8ebd911df3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
